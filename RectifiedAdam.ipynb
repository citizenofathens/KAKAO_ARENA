{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tf.Tensor()\n",
    "\n",
    "\n",
    "beta1 = tf.Variable(1. , tf.dtypes.float16 ,shape=tf.TensorShape(None))\n",
    "beta2 =\n",
    "step =\n",
    "\n",
    "(1 - )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4275515357.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"C:\\Users\\DATA\\AppData\\Local\\Temp\\ipykernel_40184\\4275515357.py\"\u001B[1;36m, line \u001B[1;32m1\u001B[0m\n\u001B[1;33m    class RectifiedAdam(tf.)\u001B[0m\n\u001B[1;37m                           ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "@tf.keras.utils.register_keras_serializable(package=\"Addons\")\n",
    "class RectifiedAdam(tf.keras.optimizers.Optimizer):\n",
    "    def __init__(self):\n",
    "        super(RectifiedAdam, self).__init__()\n",
    "\n",
    "\n",
    "        self._set_hyper()\n",
    "        self._decayed_lr()\n",
    "\n",
    "\n",
    "    # Optimizer 상속 받은 메서드들 활용 add_slot, set_hyper\n",
    "    def _create_slots(self, var_list):\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, \"m\")\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, \"v\")\n",
    "        if self.amsgrad:\n",
    "            for var in var_list:\n",
    "                self.add_slot(var, \"vhat\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RectifiedAdam(tf.keras.optimizers.Optimizer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(name, **kwargs)\n",
    "\n",
    "    # m_zero, v_zero moving 1st, 2nd moment\n",
    "    # p_inf <- 2/(1-beta2)-1  Simple Moving-Average 의 최대 길이를 계산\n",
    "    from typing import List\n",
    "\n",
    "    def _single_tensor_radam(self , params:List[tf.Tensor]\n",
    "                                   , grads:List[tf.Tensor]\n",
    "                                   , exp_avgs:List[tf.Tensor]\n",
    "                                    , exp_avg_sqs:List[tf.Tensor]\n",
    "                                    ,state_steps:List[tf.Tensor]\n",
    "                                    ,*\n",
    "                             , beta1 :tf.float16\n",
    "                             , beta2  :tf.float16\n",
    "                             ,lr : tf.float16\n",
    "                             ,weight_decay: tf.float\n",
    "                             ,eps:float ):\n",
    "\n",
    "        for i, param in enumerate(params):\n",
    "            grads = grads[i]\n",
    "            exp_avg = exp_avgs[i]\n",
    "            exp_avg_sq = exp_avg_sqs[i]\n",
    "            step_t = state_steps[i]\n",
    "\n",
    "            # update step\n",
    "            step_t +=1\n",
    "            # tensor item\n",
    "            step = step_t.items()\n",
    "\n",
    "            # beta2  (1-beta2)*(grads**2)\n",
    "            # gradient = func*(param*step)\n",
    "            # v_zero  = beta2*(v_zero*step) + (1-beta2)*(grad*step **2) 이전 시점 v_zero\n",
    "\n",
    "            # 최초  exp_avg (지수이동평균값을 parameter로 받으므로 받은 후 exp_avg t시점은 t+1 시점을 만든다)\n",
    "\n",
    "            bias_correction1 = 1-beta1**step\n",
    "            bias_correction2=  1- beta2 **step\n",
    "\n",
    "            if weight_decay != 0:\n",
    "                # torch : grad = grad.add(param, alpha=weight_decay)\n",
    "                grad =  grad+ (weight_decay*param)\n",
    "\n",
    "\n",
    "            exp_avg = tf.math.multiply(exp_avg,beta1) + ( 1- beta1)*grads\n",
    "            \"\"\"\n",
    "             m = self.get_slot(var, \"m\")\n",
    "            m_scaled_g_values = grad * coef[\"one_minus_beta_1_t\"]\n",
    "            m_t = m.assign(m * coef[\"beta_1_t\"], use_locking=self._use_locking)\n",
    "            with tf.control_dependencies([m_t]):\n",
    "                m_t = self._resource_scatter_add(m, indices, m_scaled_g_values)\n",
    "            m_corr_t = m_t * coef[\"recip_one_minus_beta_1_power\"]\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            exp_avg_sq = tf.math.multiply( exp_avg_sq*beta2) + (1-beta2) * tf.math.multiply(gradsgrads)\n",
    "            \"\"\"\n",
    "            이 한줄과 같은 코드인데 slot 을 가져오고 앞의 항과 뒤의 항을 따로 만든 뒤\n",
    "            control_depenencies context 내에서 _resource_scatter_add 라는 메서드로 + 를 해야하는 듯 하다\n",
    "            v = self.get_slot(var, \"v\")\n",
    "            v_scaled_g_values = (grad * grad) * coef[\"one_minus_beta_2_t\"]\n",
    "            v_t = v.assign(v * coef[\"beta_2_t\"], use_locking=self._use_locking)\n",
    "            with tf.control_dependencies([v_t]):\n",
    "                v_t = self._resource_scatter_add(v, indices, v_scaled_g_values)\n",
    "            \"\"\"\n",
    "\n",
    "            # Decay the first and second moment running average coefficient\n",
    "            #bias_correction = bias_correction / (1-beta1**eps)\n",
    "\n",
    "            # decay rate에 step을 제곱하여 1에서 뺀값으로 나누어\n",
    "            # 지수이동평균을 나누어 편향을 수정한다\n",
    "            bias_corrected_exp_avgs = exp_avgs/bias_correction1\n",
    "            # 지수이동평균 추정치 길이의 최대값\n",
    "            p_inf = 2/(1-beta2)-1\n",
    "\n",
    "            # compute the length of the approximated SMA 지수이동평균 길이의 추정치를 계산\n",
    "            p_t =  p_inf-2*step*(beta2**step) / bias_correction2\n",
    "\n",
    "            #\n",
    "#            bias_correction =\n",
    "\n",
    "            # 음수 값이 되지 않는선\n",
    "            if p_t > 5:\n",
    "                # Compute adaptive learning rate\n",
    "                lr = tf.math.sqrt(bias_correction2)/tf.math.sqrt(exp_avg_sqs+eps )\n",
    "                # Comput the variance reccitfication term\n",
    "                rectification_term = tf.math.sqrt((p_t -4)(p_t-2) *p_inf / ((p_inf-4)(p_inf-2)*p_t))\n",
    "\n",
    "                # add_(input, other, alpha) -> input+ (other*alpha)\n",
    "                # param.add_(bias_corrected_exp_avg * lr , alpha=-1.0)\n",
    "                # === param - bias_corrected_exp_avg *lr\n",
    "                alpha=-1\n",
    "                param=  param -( rectification_term * bias_corrected_exp_avgs*lr*alpha)\n",
    "                # 이것 자체로 param 은 t시점이된다\n",
    "            else:\n",
    "                param=  param -( bias_corrected_exp_avgs*lr*alpha)\n",
    "\n",
    "\"\"\"\n",
    "tensorflow 구현\n",
    " def _resource_apply_dense(self, grad, var, apply_state=None):\n",
    "        var_device, var_dtype = var.device, var.dtype.base_dtype\n",
    "        coef = (apply_state or {}).get(\n",
    "            (var_device, var_dtype)\n",
    "        ) or self._fallback_apply_state(var_device, var_dtype)\n",
    "        m = self.get_slot(var, \"m\")\n",
    "        v = self.get_slot(var, \"v\")\n",
    "\n",
    "        m_t = m.assign(\n",
    "            coef[\"beta_1_t\"] * m + coef[\"one_minus_beta_1_t\"] * grad,\n",
    "            use_locking=self._use_locking,\n",
    "        )\n",
    "        m_corr_t = m_t * coef[\"recip_one_minus_beta_1_power\"]\n",
    "\n",
    "        v_t = v.assign(\n",
    "            coef[\"beta_2_t\"] * v + coef[\"one_minus_beta_2_t\"] * tf.square(grad),\n",
    "            use_locking=self._use_locking,\n",
    "        )\n",
    "        if self.amsgrad:\n",
    "            vhat = self.get_slot(var, \"vhat\")\n",
    "            vhat_t = vhat.assign(tf.maximum(vhat, v_t), use_locking=self._use_locking)\n",
    "            v_corr_t = tf.sqrt(vhat_t * coef[\"recip_one_minus_beta_2_power\"])\n",
    "        else:\n",
    "            vhat_t = None\n",
    "            v_corr_t = tf.sqrt(v_t * coef[\"recip_one_minus_beta_2_power\"])\n",
    "\n",
    "        var_t = tf.where(\n",
    "            coef[\"sma_t_ge_sma_threshold\"],\n",
    "            coef[\"r_t\"] * m_corr_t / (v_corr_t + coef[\"epsilon_t\"]),\n",
    "            m_corr_t,\n",
    "        )\n",
    "\n",
    "        if self._has_weight_decay:\n",
    "            var_t += coef[\"wd_t\"] * var\n",
    "\n",
    "        var_update = var.assign_sub(coef[\"lr_t\"] * var_t, use_locking=self._use_locking)\n",
    "\n",
    "        updates = [var_update, m_t, v_t]\n",
    "        if self.amsgrad:\n",
    "            updates.append(vhat_t)\n",
    "        return tf.group(*updates)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "pytorch 구현\n",
    "def _single_tensor_radam(params: List[Tensor],\n",
    "                         grads: List[Tensor],\n",
    "                         exp_avgs: List[Tensor],\n",
    "                         exp_avg_sqs: List[Tensor],\n",
    "                         state_steps: List[Tensor],\n",
    "                         *,\n",
    "                         beta1: float,\n",
    "                         beta2: float,\n",
    "                         lr: float,\n",
    "                         weight_decay: float,\n",
    "                         eps: float):\n",
    "\n",
    "    for i, param in enumerate(params):\n",
    "        grad = grads[i]\n",
    "        exp_avg = exp_avgs[i]\n",
    "        exp_avg_sq = exp_avg_sqs[i]\n",
    "        step_t = state_steps[i]\n",
    "        # update step\n",
    "        step_t += 1\n",
    "        step = step_t.item()\n",
    "\n",
    "        bias_correction1 = 1 - beta1 ** step\n",
    "        bias_correction2 = 1 - beta2 ** step\n",
    "\n",
    "        if weight_decay != 0:\n",
    "            grad = grad.add(param, alpha=weight_decay)\n",
    "\n",
    "        # Decay the first and second moment running average coefficient\n",
    "        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "        exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "        # correcting bias for the first moving moment\n",
    "        bias_corrected_exp_avg = exp_avg / bias_correction1\n",
    "\n",
    "        # maximum length of the approximated SMA\n",
    "        rho_inf = 2 / (1 - beta2) - 1\n",
    "        # compute the length of the approximated SMA\n",
    "        rho_t = rho_inf - 2 * step * (beta2 ** step) / bias_correction2\n",
    "\n",
    "        if rho_t > 5.:\n",
    "            # Compute the variance rectification term and update parameters accordingly\n",
    "            rect = math.sqrt((rho_t - 4) * (rho_t - 2) * rho_inf / ((rho_inf - 4) * (rho_inf - 2) * rho_t))\n",
    "            adaptive_lr = math.sqrt(bias_correction2) / exp_avg_sq.sqrt().add_(eps)\n",
    "\n",
    "            param.add_(bias_corrected_exp_avg * lr * adaptive_lr * rect, alpha=-1.0)\n",
    "        else:\n",
    "            param.add_(bias_corrected_exp_avg * lr, alpha=-1.0)\n",
    "\n",
    "\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}